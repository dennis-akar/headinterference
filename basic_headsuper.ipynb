{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "https://docs.google.com/document/d/1n4h7tlNEn5xHfoDsRv5K6mpLkKRjYsBUtTXS7-KpchA\n",
    "\n",
    "0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11 as tokens.\n",
    "\n",
    "11 as bos.\n",
    "\n",
    "assume length is always the same.\n",
    "\n",
    "identity matrix as embedding and unembedding.\n",
    "\n",
    "5 attention heads, one layer. (probably superposition)\n",
    "\n",
    "12 basis residual stream.\n",
    "\"\"\"\n",
    "\n",
    "# imports\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "t.manual_seed(0)\n",
    "\n",
    "from transformer_lens.utils import get_corner, gelu_new, tokenize_and_concatenate\n",
    "\n",
    "import einops as eo\n",
    "from einops import einsum as es\n",
    "\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "from jaxtyping import Float, Int\n",
    "from torch import Tensor\n",
    "device = t.device(\"cuda\") if t.cuda.is_available() else t.device(\"cpu\")\n",
    "\n",
    "if \"pls_restart\" in globals() and eval(\"pls_restart\"):\n",
    "    print(\"pls_restart\" )\n",
    "    print(\"now i die bye have a nice day\")\n",
    "    exit()\n",
    "pls_restart = False\n",
    "\n",
    "\n",
    "# dataset\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class dataset:\n",
    "\n",
    "    # config\n",
    "\n",
    "    number_of_prompts = 64 * 64\n",
    "    fraction_of_skip_trigrams = 0.3\n",
    "    assert 0 <= fraction_of_skip_trigrams <= 1\n",
    "    length_of_prompts = 5\n",
    "    length_of_prompts = 5\n",
    "    assert length_of_prompts >= 3\n",
    "\n",
    "    skip_trigram_attend_from_token = t.tensor([0])\n",
    "    skip_trigram_attend_to_tokens = t.tensor([i for i in range(1, 6)])\n",
    "    skip_trigram_output_tokens = t.tensor([i for i in range(6, 11)])\n",
    "    other_tokens = t.tensor([])\n",
    "    bos_token = t.tensor([11])\n",
    "\n",
    "    # process\n",
    "\n",
    "    skip_trigram_count = skip_trigram_attend_to_tokens.shape[0]\n",
    "    assert skip_trigram_attend_to_tokens.ndim == skip_trigram_output_tokens.ndim == 1\n",
    "    assert skip_trigram_count == skip_trigram_output_tokens.shape[0]\n",
    "\n",
    "    non_bos_tokens = t.concat((skip_trigram_attend_from_token, skip_trigram_attend_to_tokens, skip_trigram_output_tokens, other_tokens))\n",
    "\n",
    "    tokens = t.concat((non_bos_tokens, bos_token))\n",
    "\n",
    "    skip_trigrams = t.empty((skip_trigram_count, 3), dtype=t.long)\n",
    "    skip_trigrams[:, 0] = skip_trigram_attend_from_token\n",
    "    skip_trigrams[:, 1] = skip_trigram_attend_to_tokens\n",
    "    skip_trigrams[:, 2] = skip_trigram_output_tokens\n",
    "\n",
    "    data = t.full((number_of_prompts, length_of_prompts), bos_token.item())\n",
    "    data[:, 1:] = non_bos_tokens[t.randint(len(non_bos_tokens), (number_of_prompts, length_of_prompts-1))]\n",
    "    \n",
    "    for i in range(number_of_prompts):\n",
    "        prob = t.rand(1)\n",
    "        if prob < fraction_of_skip_trigrams:\n",
    "            data[i, (1, -2, -1)] = skip_trigrams[int(prob * skip_trigram_count), :]\n",
    "    del prob, i\n",
    "dataset = dataset()\n",
    "\n",
    "\n",
    "# transformer\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class transformer_cfg:\n",
    "    d_model = 12\n",
    "    assert type(d_model) == int and d_model > 0\n",
    "    \n",
    "    debug = True\n",
    "    assert type(debug) == bool\n",
    "    \n",
    "    layer_norm_eps = 1e-5\n",
    "    assert type(layer_norm_eps) == float and layer_norm_eps > 0.0\n",
    "    \n",
    "    d_vocab = len(dataset.tokens)\n",
    "    assert type(d_vocab) == int and d_vocab > 0\n",
    "\n",
    "    embed_unembed_eye = True\n",
    "    assert type(embed_unembed_eye) == bool\n",
    "    if embed_unembed_eye:\n",
    "        assert d_vocab <= d_model\n",
    "    \n",
    "    init_range = 0.02\n",
    "    assert type(init_range) == float and layer_norm_eps > 0.0\n",
    "    \n",
    "    n_ctx = dataset.length_of_prompts\n",
    "    assert type(n_ctx) == int and n_ctx > 0\n",
    "    \n",
    "    d_head = 3\n",
    "    assert type(d_head) == int and d_head >= 0\n",
    "    \n",
    "    d_mlp = 0\n",
    "    assert type(d_mlp) == int and d_mlp >= 0\n",
    "    \n",
    "    n_heads = 5\n",
    "    assert type(n_heads) == int and n_heads >= 0\n",
    "    \n",
    "    n_layers = 1\n",
    "    assert type(n_layers) == int and n_layers >= 0\n",
    "transformer_cfg = transformer_cfg()\n",
    "\n",
    "# @dataclass(frozen=True)\n",
    "# class transformer_cfg:\n",
    "#     d_model: int = 768\n",
    "#     assert d_model > 0\n",
    "#     debug: bool = True\n",
    "#     layer_norm_eps: float = 1e-5\n",
    "#     d_vocab: int = 50257\n",
    "#     init_range: float = 0.02\n",
    "#     n_ctx: int = 1024\n",
    "#     d_head: int = 64\n",
    "#     d_mlp: int = 3072\n",
    "#     n_heads: int = 12\n",
    "#     n_layers: int = 12\n",
    "# transformer_cfg = transformer_cfg()\n",
    "\n",
    "\n",
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if cfg.embed_unembed_eye:\n",
    "            assert cfg.d_vocab <= cfg.d_model\n",
    "            self.W_E = t.eye(cfg.d_model).to(device)\n",
    "        else:\n",
    "            self.W_E = nn.Parameter(t.empty((cfg.d_vocab, cfg.d_model)))\n",
    "            nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        return self.W_E[tokens]\n",
    "\n",
    "\n",
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(t.empty((cfg.n_ctx, cfg.d_model)))\n",
    "        nn.init.normal_(self.W_pos, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        batch, seq_len = tokens.shape\n",
    "        return eo.repeat(self.W_pos[:seq_len], \"seq d_model -> batch seq d_model\", batch=batch)\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(t.ones(cfg.d_model))\n",
    "        self.b = nn.Parameter(t.zeros(cfg.d_model))\n",
    "\n",
    "    def forward(self, residual: Float[Tensor, \"batch posn d_model\"]) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        residual_mean = residual.mean(dim=-1, keepdim=True)\n",
    "        residual_std = (residual.var(dim=-1, keepdim=True, unbiased=False) + self.cfg.layer_norm_eps).sqrt()\n",
    "\n",
    "        residual = (residual - residual_mean) / residual_std\n",
    "        return residual * self.w + self.b\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    IGNORE: Float[Tensor, \"\"]\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_Q = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_K = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_V = nn.Parameter(t.empty((cfg.n_heads, cfg.d_model, cfg.d_head)))\n",
    "        self.W_O = nn.Parameter(t.empty((cfg.n_heads, cfg.d_head, cfg.d_model)))\n",
    "        self.b_Q = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_K = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_V = nn.Parameter(t.zeros((cfg.n_heads, cfg.d_head)))\n",
    "        self.b_O = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_Q, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_K, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_V, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_O, std=self.cfg.init_range)\n",
    "        self.register_buffer(\"IGNORE\", t.tensor(-1e5, dtype=t.float32, device=device))\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_pre: Float[Tensor, \"batch posn d_model\"]\n",
    "    ) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        # Calculate query, key and value vectors\n",
    "        q = es(\n",
    "            normalized_resid_pre, self.W_Q,\n",
    "            \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\", \n",
    "        ) + self.b_Q\n",
    "        k = es(\n",
    "            normalized_resid_pre, self.W_K,\n",
    "            \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\", \n",
    "        ) + self.b_K\n",
    "        v = es(\n",
    "            normalized_resid_pre, self.W_V,\n",
    "            \"batch posn d_model, nheads d_model d_head -> batch posn nheads d_head\", \n",
    "        ) + self.b_V\n",
    "\n",
    "        # Calculate attention scores, then scale and mask, and apply softmax to get probabilities\n",
    "        attn_scores = es(\n",
    "            q, k,\n",
    "            \"batch posn_Q nheads d_head, batch posn_K nheads d_head -> batch nheads posn_Q posn_K\", \n",
    "        )\n",
    "        attn_scores_masked = self.apply_causal_mask(attn_scores / self.cfg.d_head ** 0.5)\n",
    "        attn_pattern = attn_scores_masked.softmax(-1)\n",
    "\n",
    "        # Take weighted sum of value vectors, according to attention probabilities\n",
    "        z = es(\n",
    "            v, attn_pattern,\n",
    "            \"batch posn_K nheads d_head, batch nheads posn_Q posn_K -> batch posn_Q nheads d_head\", \n",
    "        )\n",
    "\n",
    "        # Calculate output (by applying matrix W_O and summing over heads, then adding bias b_O)\n",
    "        attn_out = es(\n",
    "            z, self.W_O,\n",
    "            \"batch posn_Q nheads d_head, nheads d_head d_model -> batch posn_Q d_model\", \n",
    "        ) + self.b_O\n",
    "\n",
    "        return attn_out\n",
    "\n",
    "    def apply_causal_mask(\n",
    "        self, attn_scores: Float[Tensor, \"batch n_heads query_pos key_pos\"]\n",
    "    ) -> Float[Tensor, \"batch n_heads query_pos key_pos\"]:\n",
    "        '''\n",
    "        Applies a causal mask to attention scores, and returns masked scores.\n",
    "        '''\n",
    "        # Define a mask that is True for all positions we want to set probabilities to zero for\n",
    "        all_ones = t.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device)\n",
    "        mask = t.triu(all_ones, diagonal=1).bool()\n",
    "        # Apply the mask to attention scores, then return the masked scores\n",
    "        attn_scores.masked_fill_(mask, self.IGNORE)\n",
    "        return attn_scores\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(t.empty((cfg.d_model, cfg.d_mlp)))\n",
    "        self.W_out = nn.Parameter(t.empty((cfg.d_mlp, cfg.d_model)))\n",
    "        self.b_in = nn.Parameter(t.zeros((cfg.d_mlp)))\n",
    "        self.b_out = nn.Parameter(t.zeros((cfg.d_model)))\n",
    "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_mid: Float[Tensor, \"batch posn d_model\"]\n",
    "    ) -> Float[Tensor, \"batch posn d_model\"]:\n",
    "        pre = es(\n",
    "            normalized_resid_mid, self.W_in,\n",
    "            \"batch position d_model, d_model d_mlp -> batch position d_mlp\", \n",
    "        ) + self.b_in\n",
    "        post = gelu_new(pre)\n",
    "        mlp_out = es(\n",
    "            post, self.W_out,\n",
    "            \"batch position d_mlp, d_mlp d_model -> batch position d_model\", \n",
    "        ) + self.b_out\n",
    "        return mlp_out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if cfg.d_head > 0 and cfg.n_heads > 0:\n",
    "            self.ln1 = LayerNorm(cfg)\n",
    "            self.attn = Attention(cfg)\n",
    "        if cfg.d_mlp > 0:\n",
    "            self.ln2 = LayerNorm(cfg)\n",
    "            self.mlp = MLP(cfg)\n",
    "\n",
    "    def forward(\n",
    "        self, resid_pre: Float[Tensor, \"batch position d_model\"]\n",
    "    ) -> Float[Tensor, \"batch position d_model\"]:\n",
    "        if self.cfg.d_head > 0 and self.cfg.n_heads > 0:\n",
    "            resid_mid = self.attn(self.ln1(resid_pre)) + resid_pre\n",
    "        else:\n",
    "            resid_mid = resid_pre\n",
    "        if self.cfg.d_mlp > 0:\n",
    "            resid_post = self.mlp(self.ln2(resid_mid)) + resid_mid\n",
    "        else:\n",
    "            resid_post = resid_mid\n",
    "        return resid_post\n",
    "\n",
    "\n",
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        if cfg.embed_unembed_eye:\n",
    "            assert cfg.d_vocab <= cfg.d_model\n",
    "            self.W_U = t.eye(cfg.d_model).to(device)\n",
    "            self.b_U = t.Tensor([0]).to(device)\n",
    "        else:\n",
    "            self.W_U = nn.Parameter(t.empty((cfg.d_model, cfg.d_vocab)))\n",
    "            nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
    "            self.b_U = nn.Parameter(t.zeros((cfg.d_vocab), requires_grad=False))\n",
    "\n",
    "    def forward(\n",
    "        self, normalized_resid_final: Float[Tensor, \"batch position d_model\"]\n",
    "    ) -> Float[Tensor, \"batch position d_vocab\"]:\n",
    "        return es(\n",
    "            normalized_resid_final, self.W_U,\n",
    "            \"batch posn d_model, d_model d_vocab -> batch posn d_vocab\",\n",
    "        ) + self.b_U\n",
    "        # Or, could just do `normalized_resid_final @ self.W_U + self.b_U`\n",
    "\n",
    "\n",
    "class DemoTransformer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(cfg) for _ in range(cfg.n_layers)])\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "\n",
    "    def forward(self, tokens: Int[Tensor, \"batch position\"]) -> Float[Tensor, \"batch position d_vocab\"]:\n",
    "        residual = self.embed(tokens) + self.pos_embed(tokens)\n",
    "        for block in self.blocks:\n",
    "            residual = block(residual)\n",
    "        logits = self.unembed(self.ln_final(residual))\n",
    "        return logits\n",
    "\n",
    "\n",
    "# train\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class training_cfg():\n",
    "    batch_size = 64 # 8\n",
    "    num_epochs = 1\n",
    "    max_steps = float(\"inf\")\n",
    "    log_every = 100\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-2\n",
    "training_cfg = training_cfg()\n",
    "\n",
    "\n",
    "# def get_log_probs(\n",
    "# \tlogits: Float[Tensor, \"batch posn d_vocab\"], \n",
    "# \ttokens: Int[Tensor, \"batch posn\"]\n",
    "# ) -> Float[Tensor, \"batch posn-1\"]:\n",
    "    \n",
    "# \tlog_probs = logits.log_softmax(dim=-1)\n",
    "# \t# Get logprobs the first seq_len-1 predictions (so we can compare them with the actual next tokens)\n",
    "# \tlog_probs_for_tokens = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "# \treturn log_probs_for_tokens\n",
    "\n",
    "def lm_cross_entropy_loss(logits, tokens):\n",
    "    # Measure next token loss\n",
    "    # Logits have shape [batch, position, d_vocab]\n",
    "    # Tokens have shape [batch, position]\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    pred_log_probs = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "    return -pred_log_probs.mean()\n",
    "\n",
    "\n",
    "# pred_log_probs = get_log_probs(demo_logits, tokens)\n",
    "# print(f\"Avg cross entropy loss: {-pred_log_probs.mean():.4f}\")\n",
    "# print(f\"Avg cross entropy loss for uniform distribution: {math.log(demo_gpt2.cfg.d_vocab):4f}\")\n",
    "# print(f\"Avg probability assigned to correct token: {pred_log_probs.exp().mean():4f}\")\n",
    "\n",
    "# dataset = datasets.load_dataset(\"NeelNanda/pile-10k\", split=\"train\").remove_columns(\"meta\")\n",
    "# print(dataset)\n",
    "# print(dataset[0]['text'][:100])\n",
    "\n",
    "model = DemoTransformer(transformer_cfg)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = t.optim.AdamW(model.parameters(), lr=training_cfg.lr, weight_decay=training_cfg.weight_decay)\n",
    "\n",
    "dataset_batched = eo.rearrange(dataset.data, \"(batch number_of_prompts) len_prompts -> batch number_of_prompts len_prompts\", batch=training_cfg.batch_size)\n",
    "\n",
    "losses = []\n",
    "print(\"Number of batches:\", dataset.data.shape[0])\n",
    "for epoch in range(training_cfg.num_epochs):\n",
    "    for c, batch in tqdm(enumerate(dataset.data)):\n",
    "        # print(batch.shape)\n",
    "        # print(batch.unsqueeze(0))\n",
    "        tokens = batch.unsqueeze(0).to(device)#['tokens'].cuda()\n",
    "        logits = model(tokens)\n",
    "        loss = lm_cross_entropy_loss(logits, tokens)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        losses.append(loss.item())\n",
    "        if c % training_cfg.log_every == 0:\n",
    "            print(f\"Step: {c}, Loss: {loss.item():.4f}\")\n",
    "        if c > training_cfg.max_steps:\n",
    "            break\n",
    "\n",
    "px.line(y=losses, x=np.arange(len(losses))*(transformer_cfg.n_ctx * training_cfg.batch_size), labels={\"y\":\"Loss\", \"x\":\"Tokens\"}, title=\"Training curve for my tiny demo model!\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze\n",
    "\n",
    "px.line(y=losses, x=np.arange(len(losses))*(transformer_cfg.n_ctx * training_cfg.batch_size), labels={\"y\":\"Loss\", \"x\":\"Tokens\"}, title=\"Training curve for my tiny demo model!\")\n",
    "\n",
    "\n",
    ""
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 }
}